{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "\n",
      " Training set Accuracy:83.434%\n",
      "\n",
      " Validation set Accuracy:83.44%Nhidden layer = 4 lambda = 0\n",
      "\n",
      " Test set Accuracy:84.09%\n",
      "########################################################################\n",
      "255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3522483fd3d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mselected_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m         \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-3522483fd3d3>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# If feature is of no importance in training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m                     \u001b[1;31m# if feature is of no importance in validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.io import loadmat\n",
    "from math import sqrt\n",
    "import pickle\n",
    "import time\n",
    "from datetime import timedelta\n",
    "lamb = [0,5,10,15,20,25,30,35,40,45,50]\n",
    "hid = [4,8,12,16,20]\n",
    "for a in lamb:\n",
    "    for b in hid:\n",
    "\n",
    "\n",
    "\n",
    "        def initializeWeights(n_in, n_out):\n",
    "            \"\"\"\n",
    "            # initializeWeights return the random weights for Neural Network given the\n",
    "            # number of node in the input layer and output layer\n",
    "            # Input:\n",
    "            # n_in: number of nodes of the input layer\n",
    "            # n_out: number of nodes of the output layer\n",
    "            # Output:\n",
    "            # W: matrix of random initial weights with size (n_out x (n_in + 1))\"\"\"\n",
    "            epsilon = sqrt(6) / sqrt(n_in + n_out + 1)\n",
    "            W = (np.random.rand(n_out, n_in + 1) * 2 * epsilon) - epsilon\n",
    "            return W\n",
    "\n",
    "        def sigmoid(z):\n",
    "            # Return the sigmoid of the function\n",
    "            return (1.0/ (1.0 + np.exp(-z)))\n",
    "\n",
    "\n",
    "        def preprocess():\n",
    "            \"\"\" Input:\n",
    "             Although this function doesn't have any input, you are required to load\n",
    "             the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "             Output:\n",
    "             train_data: matrix of training set. Each row of train_data contains \n",
    "               feature vector of a image\n",
    "             train_label: vector of label corresponding to each image in the training\n",
    "               set\n",
    "             validation_data: matrix of training set. Each row of validation_data \n",
    "               contains feature vector of a image\n",
    "             validation_label: vector of label corresponding to each image in the \n",
    "               training set\n",
    "             test_data: matrix of training set. Each row of test_data contains \n",
    "               feature vector of a image\n",
    "             test_label: vector of label corresponding to each image in the testing\n",
    "               set\n",
    "\n",
    "             Some suggestions for preprocessing step:\n",
    "             - feature selection\"\"\"\n",
    "\n",
    "            mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "            \n",
    "            print(max(mat[\"train0\"][1]))\n",
    "\n",
    "            # Split the training sets into two sets of 50000 randomly sampled training examples and 10000 validation examples. \n",
    "            # Your code here.\n",
    "            train_data_pre = np.zeros(shape=(50000, len(mat[\"train0\"][1])))\n",
    "            validation_data_pre = np.zeros(shape=(10000, len(mat[\"train0\"][1])))\n",
    "            test_data_pre = np.zeros(shape=(10000, len(mat[\"train0\"][1])))\n",
    "            train_label_pre = np.zeros(shape=(50000,))\n",
    "            validation_label_pre = np.zeros(shape=(10000,))\n",
    "            test_label_pre = np.zeros(shape=(10000,))\n",
    "            train_len = 0\n",
    "            validation_len = 0\n",
    "            test_len = 0\n",
    "            train_label_len = 0\n",
    "            validation_label_len = 0\n",
    "\n",
    "            for i in mat:\n",
    "                if \"train\" in  i:\n",
    "                    value = mat[i]\n",
    "                    d = len(value)\n",
    "                    leng = d - 1000\n",
    "                    train_data_pre[train_len:train_len+leng] = value[1000:]\n",
    "                    train_len += leng\n",
    "                    #print(train_len)\n",
    "\n",
    "                    train_label_pre[train_label_len:train_label_len + leng] = i[-1]\n",
    "                    #print(train_label_pre)\n",
    "                    train_label_len += leng\n",
    "\n",
    "                    validation_data_pre[validation_len:validation_len + 1000] = value[0:1000]\n",
    "                    validation_len += 1000\n",
    "\n",
    "                    validation_label_pre[validation_label_len:validation_label_len + 1000] = i[-1]\n",
    "                    #print(validation_label_pre)\n",
    "                    validation_label_len += 1000\n",
    "                if \"test\" in i:\n",
    "                    value = mat[i]\n",
    "                    d = len(value)\n",
    "                    test_label_pre[test_len:test_len + d] = i[-1]\n",
    "                    test_data_pre[test_len:test_len + d] = value\n",
    "                    #print(test_data_pre)\n",
    "                    test_len += d\n",
    "\n",
    "           ######################### Shuffle and Normalize###############################################\n",
    "\n",
    "            train_size = len(train_data_pre)\n",
    "            train_perm = np.random.permutation(train_size)\n",
    "            train_data = train_data_pre[train_perm]\n",
    "            train_data = (train_data - min(mat[\"train0\"][1])) / (max(mat[\"train0\"][1]) - min(mat[\"train0\"][1])) ##Normalize\n",
    "            train_data = np.double(train_data)\n",
    "            train_label = train_label_pre[train_perm]\n",
    "\n",
    "            validation_size = len(validation_data_pre)\n",
    "            vali_perm = np.random.permutation(validation_size)\n",
    "            validation_data = validation_data_pre[vali_perm]\n",
    "            validation_data = (validation_data - min(mat[\"train0\"][1])) / (max(mat[\"train0\"][1]) - min(mat[\"train0\"][1]))\n",
    "            validation_data = np.double(validation_data)\n",
    "            validation_label = validation_label_pre[vali_perm]\n",
    "\n",
    "            test_size = len(validation_data_pre)\n",
    "            test_perm = np.random.permutation(test_size)\n",
    "            test_data = test_data_pre[test_perm]\n",
    "            test_data = (test_data - min(mat[\"train0\"][1])) / (max(mat[\"train0\"][1]) - min(mat[\"train0\"][1]))\n",
    "            test_data = np.double(test_data)\n",
    "            test_label = test_label_pre[test_perm]\n",
    "\n",
    "            features_to_delete = []\n",
    "\n",
    "\n",
    "\n",
    "            #print(train_data.shape)        \n",
    "\n",
    "\n",
    "\n",
    "            ####### Deleting uselesss features ##########################\n",
    "            for i in range(len(mat[\"train0\"][1])):\n",
    "                # If feature is of no importance in training data\n",
    "                if max(train_data[:,i]) - min(train_data[:,i])  == 0:\n",
    "                    # if feature is of no importance in validation data\n",
    "                    if max(validation_data[:,i]) - min(validation_data[:,i]) == 0:\n",
    "                        # same check on test data\n",
    "                        if max(test_data[:,i]) - min(test_data[:,i]) == 0:\n",
    "                            #print(1)\n",
    "                            features_to_delete.append(i)\n",
    "                else:\n",
    "                    selected_feature.append(i)\n",
    "\n",
    "\n",
    "\n",
    "            train_data = np.delete(train_data, features_to_delete, axis=1)\n",
    "            validation_data = np.delete(validation_data, features_to_delete, axis=1)\n",
    "            test_data = np.delete(test_data, features_to_delete, axis=1)\n",
    "\n",
    "\n",
    "            #print('preprocess done')\n",
    "\n",
    "            return train_data, train_label, validation_data, validation_label, test_data, test_label\n",
    "\n",
    "\n",
    "        def nnObjFunction(params, *args):\n",
    "            \"\"\"% nnObjFunction computes the value of objective function (negative log\n",
    "            %   likelihood error function with regularization) given the parameters\n",
    "            %   of Neural Networks, thetraining data, their corresponding training\n",
    "            %   labels and lambda - regularization hyper-parameter.\n",
    "            % Input:\n",
    "            % params: vector of weights of 2 matrices w1 (weights of connections from\n",
    "            %     input layer to hidden layer) and w2 (weights of connections from\n",
    "            %     hidden layer to output layer) where all of the weights are contained\n",
    "            %     in a single vector.\n",
    "            % n_input: number of node in input layer (not include the bias node)\n",
    "            % n_hidden: number of node in hidden layer (not include the bias node)\n",
    "            % n_class: number of node in output layer (number of classes in\n",
    "            %     classification problem\n",
    "            % training_data: matrix of training data. Each row of this matrix\n",
    "            %     represents the feature vector of a particular image\n",
    "            % training_label: the vector of truth label of training images. Each entry\n",
    "            %     in the vector represents the truth label of its corresponding image.\n",
    "            % lambda: regularization hyper-parameter. This value is used for fixing the\n",
    "            %     overfitting problem.\n",
    "            % Output:\n",
    "            % obj_val: a scalar value representing value of error function\n",
    "            % obj_grad: a SINGLE vector of gradient value of error function\n",
    "            % NOTE: how to compute obj_grad\n",
    "            % Use backpropagation algorithm to compute the gradient of error function\n",
    "            % for each weights in weight matrices.\n",
    "            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            % reshape 'params' vector into 2 matrices of weight w1 and w2\n",
    "            % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "            %     w1(i, j) represents the weight of connection from unit j in input\n",
    "            %     layer to unit i in hidden layer.\n",
    "            % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "            %     w2(i, j) represents the weight of connection from unit j in hidden\n",
    "            %     layer to unit i in output layer.\"\"\"\n",
    "\n",
    "            n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "\n",
    "            w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "            w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "            obj_val = 0\n",
    "\n",
    "            # Add Bias\n",
    "            training_data = np.column_stack((training_data,np.ones(train_data.shape[0])))\n",
    "\n",
    "            ####### Forward Pass\n",
    "\n",
    "\n",
    "\n",
    "            ######### Hidden Layer\n",
    "            hiddenOutput = np.dot(training_data, w1.T)\n",
    "            hiddenOutput = sigmoid(hiddenOutput)\n",
    "            # Bias\n",
    "            hiddenOutput = np.column_stack((hiddenOutput,np.ones(train_data.shape[0])))  #add column  \n",
    "            ##### Output Layer\n",
    "\n",
    "            Finaloutput = np.dot(hiddenOutput, w2.T)\n",
    "\n",
    "            Finaloutput=sigmoid(Finaloutput)\n",
    "\n",
    "            Class = np.zeros((training_data.shape[0],n_class)) \n",
    "\n",
    "\n",
    "            for i in range(training_label.shape[0]):\n",
    "                position = int(training_label[i])\n",
    "                Class[i][position] = 1\n",
    "\n",
    "            # Starting the backward pass\n",
    "            size = training_data.shape[0]\n",
    "            ##################### Lecture 9 ################################\n",
    "            ###Error Function\n",
    "            J = ((Class * np.log(Finaloutput)) + ((1 - Class) * np.log((1 - Finaloutput))))\n",
    "            error_func = ((-1) * np.sum(J))/size\n",
    "\n",
    "\n",
    "\n",
    "            ####### Regularization\n",
    "\n",
    "            Reg = (np.sum(w1**2) + np.sum(w2**2)) * (lambdaval/(2*size))\n",
    "\n",
    "            obj_val = error_func + Reg\n",
    "\n",
    "            #################Gradient####################\n",
    "            ########## Lecture 9 Handouts############################\n",
    "            delta = Finaloutput - Class\n",
    "\n",
    "            grad_w2 = np.dot(((delta).T), hiddenOutput) \n",
    "\n",
    "            grad_w2 = (grad_w2+(lambdaval*w2))/size\n",
    "\n",
    "\n",
    "\n",
    "            grad_w1 = np.dot(((1-hiddenOutput)*hiddenOutput* (np.dot(delta,w2))).T,training_data)\n",
    "            grad_w1 = np.delete(grad_w1, n_hidden, axis=0)\n",
    "            grad_w1 = (grad_w1 + (lambdaval * w1)) /size\n",
    "            #print(grad_w1)\n",
    "\n",
    "            obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "\n",
    "            return (obj_val, obj_grad)\n",
    "\n",
    "\n",
    "\n",
    "        def nnPredict(w1, w2, data):\n",
    "            \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n",
    "            % Network.\n",
    "            % Input:\n",
    "            % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "            %     w1(i, j) represents the weight of connection from unit i in input\n",
    "            %     layer to unit j in hidden layer.\n",
    "            % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "            %     w2(i, j) represents the weight of connection from unit i in input\n",
    "            %     layer to unit j in hidden layer.\n",
    "            % data: matrix of data. Each row of this matrix represents the feature\n",
    "            %       vector of a particular image\n",
    "            % Output:\n",
    "            % label: a column vector of predicted labels\"\"\"\n",
    "\n",
    "            \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n",
    "            % Network.\n",
    "            % Input:\n",
    "            % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "            %     w1(i, j) represents the weight of connection from unit i in input\n",
    "            %     layer to unit j in hidden layer.\n",
    "            % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "            %     w2(i, j) represents the weight of connection from unit i in input\n",
    "            %     layer to unit j in hidden layer.\n",
    "            % data: matrix of data. Each row of this matrix represents the feature\n",
    "            %       vector of a particular image\n",
    "\n",
    "            % Output: \n",
    "            % label: a column vector of predicted labels\"\"\" \n",
    "            # Number of Items  \n",
    "            labels = np.array([])\n",
    "            # Your code here\n",
    "            data = np.column_stack((data,np.ones(data.shape[0])))\n",
    "            # Ïƒ(w^t * x )\n",
    "            hidden_layer_output = sigmoid(np.dot(data, w1.T))\n",
    "            hidden_layer_output = np.column_stack((hidden_layer_output, np.ones(hidden_layer_output.shape[0])))\n",
    "            output_layer_output = sigmoid(np.dot(hidden_layer_output, w2.T))\n",
    "            labels = np.argmax(output_layer_output, axis=1)\n",
    "            return labels\n",
    "\n",
    "\n",
    "        \"\"\"**************Neural Network Script Starts here********************************\"\"\"\n",
    "        \n",
    "        # Start-time used for printing time-usage below.\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        selected_feature = []\n",
    "        train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "\n",
    "        #  Train Neural Network\n",
    "\n",
    "        # set the number of nodes in input unit (not including bias unit)\n",
    "        n_input = train_data.shape[1]\n",
    "\n",
    "        # set the number of nodes in hidden unit (not including bias unit)\n",
    "        n_hidden = b\n",
    "\n",
    "\n",
    "        # set the number of nodes in output unit\n",
    "        n_class = 10\n",
    "\n",
    "        # initialize the weights into some random matrices\n",
    "        initial_w1 = initializeWeights(n_input, n_hidden)\n",
    "        initial_w2 = initializeWeights(n_hidden, n_class)\n",
    "\n",
    "        # unroll 2 weight matrices into single column vector\n",
    "        initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)\n",
    "\n",
    "        # set the regularization hyper-parameter\n",
    "        lambdaval = a\n",
    "\n",
    "\n",
    "        args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "        # Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "        opts = {'maxiter': 50}  # Preferred value.\n",
    "\n",
    "        nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)\n",
    "\n",
    "        # In Case you want to use fmin_cg, you may have to split the nnObjectFunction to two functions nnObjFunctionVal\n",
    "        # and nnObjGradient. Check documentation for this function before you proceed.\n",
    "        # nn_params, cost = fmin_cg(nnObjFunctionVal, initialWeights, nnObjGradient,args = args, maxiter = 50)\n",
    "\n",
    "\n",
    "        # Reshape nnParams from 1D vector into w1 and w2 matrices\n",
    "\n",
    "        w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "        w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "        # Test the computed parameters\n",
    "\n",
    "        predicted_label = nnPredict(w1, w2, train_data)\n",
    "\n",
    "        # find the accuracy on Training Dataset\n",
    "\n",
    "        print('\\n Training set Accuracy:' + str(100 * np.mean((predicted_label == train_label).astype(float))) + '%')\n",
    "\n",
    "        predicted_label = nnPredict(w1, w2, validation_data)\n",
    "\n",
    "        # find the accuracy on Validation Dataset\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Difference between start and end-times.\n",
    "        time_dif = end_time - start_time\n",
    "\n",
    "        # Print the time-usage.\n",
    "        #print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "        print('\\n Validation set Accuracy:' + str(100 * np.mean((predicted_label == validation_label).astype(float))) + '%' + 'Nhidden layer = ' + str(b), \"lambda = \" + str(a))\n",
    "\n",
    "\n",
    "\n",
    "        predicted_label = nnPredict(w1, w2, test_data)\n",
    "\n",
    "        # find the accuracy on Validation Dataset\n",
    "\n",
    "        print('\\n Test set Accuracy:' + str(100 * np.mean((predicted_label == test_label).astype(float))) + '%')\n",
    "        print(\"########################################################################\")\n",
    "\n",
    "\n",
    "        pickle.dump((selected_feature,n_hidden,w1,w2,lambdaval),open('params.pickle','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  149,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  247,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  259,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  318,\n",
       "  319,\n",
       "  320,\n",
       "  321,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  340,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  477,\n",
       "  478,\n",
       "  479,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  495,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  571,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  578,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  582,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  646,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  658,\n",
       "  659,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  691,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  707,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  717,\n",
       "  718,\n",
       "  719,\n",
       "  720,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  726,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  740,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  760,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779],\n",
       " 20,\n",
       " array([[-6.39679859e-03,  1.45829945e-02, -1.09399478e-02, ...,\n",
       "          3.82976795e-03, -1.35215936e-02, -1.54595434e-01],\n",
       "        [-5.93061543e-03,  9.93352126e-03, -4.07658817e-05, ...,\n",
       "          2.90432667e-03,  9.54397931e-03,  2.89541366e-01],\n",
       "        [ 7.18860496e-04, -1.59039714e-02, -2.02593791e-03, ...,\n",
       "         -7.18389255e-03,  5.19222392e-03,  1.76443459e-01],\n",
       "        ...,\n",
       "        [-8.53805477e-05, -9.37256732e-03, -1.21664299e-02, ...,\n",
       "         -2.56662528e-03, -6.92675923e-03,  2.98623326e-01],\n",
       "        [-1.99817333e-03,  4.39198182e-03, -4.86885214e-03, ...,\n",
       "          4.84032854e-03,  8.31750015e-04,  6.31757617e-01],\n",
       "        [ 1.12240115e-02, -1.96384342e-03, -5.86714523e-03, ...,\n",
       "          3.28951414e-03, -5.02343952e-03,  1.28573222e-01]]),\n",
       " array([[ 0.7781994 ,  1.72560658, -1.35604857, -1.63113773,  0.45483592,\n",
       "          1.91726372, -0.62883181, -1.1406442 , -0.20363769, -2.87432605,\n",
       "         -1.46737079, -0.94441123, -0.99688893, -0.97767824,  1.25827637,\n",
       "         -1.17449921,  0.53451772, -0.63881776,  0.35748619,  0.53425429,\n",
       "         -1.16039625],\n",
       "        [ 1.16554721, -0.43670028,  0.32437407, -0.9423169 , -1.40684474,\n",
       "         -1.31989769,  2.91732316, -0.53225761, -2.11438683, -0.14837572,\n",
       "          0.87516581,  0.01294282, -1.76625517,  2.11168375, -1.12600709,\n",
       "         -0.94393978, -1.00461283,  0.577159  , -1.29021705,  0.80834757,\n",
       "         -0.9716867 ],\n",
       "        [-0.01802821, -1.29190575,  1.90437946, -2.58689128, -1.633203  ,\n",
       "          1.20460523, -1.53922653, -0.45285397, -1.89418232,  2.12865049,\n",
       "         -1.56449999, -1.15232739, -0.2964785 , -0.81308703,  1.91662   ,\n",
       "         -0.56157742, -0.58080933,  0.05394762, -1.5686483 , -0.79089382,\n",
       "         -0.54799043],\n",
       "        [-0.22314951, -1.48468144, -0.7874295 ,  1.38947707, -1.41729649,\n",
       "          2.09037672, -1.77387503, -1.54421899, -1.35085818,  0.41406323,\n",
       "          2.72284058,  0.09164726, -0.9307057 , -0.6793501 , -1.35503708,\n",
       "         -1.32622608, -1.15363539, -0.39067913,  2.05492756, -1.3659508 ,\n",
       "         -1.2539405 ],\n",
       "        [-2.02781928, -1.91905451,  2.90358586, -0.23603869,  1.43592142,\n",
       "         -0.76777528,  0.3798083 ,  1.97998799,  0.98437676, -2.62044959,\n",
       "         -0.39263899,  0.44962002, -0.88484609, -1.4720625 , -0.67685284,\n",
       "         -0.92102531, -0.72205197, -0.69350234, -0.30588839, -0.98581793,\n",
       "         -1.01711824],\n",
       "        [ 0.76585253,  0.84094576,  0.01252499,  1.19926887,  1.97287994,\n",
       "         -2.87576587, -0.69152904, -1.29933493, -0.81939763, -0.84738015,\n",
       "         -1.27334165, -2.09420516, -1.85535897,  0.53378743, -2.07092986,\n",
       "         -0.72179228, -0.86649725,  1.56921543,  2.16208351, -1.02771997,\n",
       "         -0.26439054],\n",
       "        [-0.9337796 , -1.49639549, -1.82271735, -0.05306663,  1.23567669,\n",
       "         -1.60538543, -0.83942522, -1.65814724,  1.49415237,  0.02098826,\n",
       "         -0.86513217, -0.10527156, -0.95890687,  2.4461707 ,  2.20567874,\n",
       "         -1.11854204, -1.28917659, -0.22275349, -1.27086301, -0.02194568,\n",
       "         -1.13801957],\n",
       "        [-2.9682772 ,  1.0983894 ,  0.13953577, -1.78564063, -1.43545137,\n",
       "         -1.1093203 , -1.07538191,  0.70546047,  0.24704728, -0.13699943,\n",
       "          2.24258256, -1.86611869, -0.64269838, -0.48410236, -1.42812879,\n",
       "         -0.47778415,  0.84478547,  0.12919451,  0.23374521,  1.88840132,\n",
       "         -0.71438744],\n",
       "        [ 0.65489791, -0.21177437, -0.55573309,  1.82374347, -0.89887827,\n",
       "         -1.56168106, -1.96776586, -2.18256162, -0.31457502, -0.08875123,\n",
       "         -1.90296566,  1.56907953, -0.05953513, -1.80969803, -2.25487356,\n",
       "         -0.43516391,  2.38431185, -1.46042938, -3.00649692, -0.67509422,\n",
       "         -0.98922549],\n",
       "        [-1.47911557,  1.39497082, -3.36349797,  0.3174092 , -2.07912837,\n",
       "          0.94696363,  1.41732732,  1.55640247,  1.07424238,  0.46410062,\n",
       "         -1.4142014 ,  1.63225052, -0.7463411 , -1.98936072, -0.79847038,\n",
       "         -0.89366511, -1.99070699, -0.06198964, -0.74152306, -1.61591701,\n",
       "         -0.94459071]]),\n",
       " 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_obj = pickle.load(file=open('params.pickle', \"rb\"))\n",
    "pickle_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
